```tex
1. Email地址：^\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*$

2. 域名：[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(/.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+/.?

3. URL：[a-zA-z]+://[^\s]* 或 ^http://([\w-]+\.)+[\w-]+(/[\w-./?%&=]*)?$

4. IP地址:\d+\.\d+\.\d+\.\d+
```

## 一、正则表达式回顾

```
单字修饰符：
	1. .   匹配任意字符，除了换行符
	2. []  用来表示一组字符,单独列出：[abc] 匹配 'a'，'b'或'c'
	3. \d  匹配任意数字，等价于 [0-9].
	4. \D  匹配任意非数字
	5. \w  匹配字母数字及下划线
	6. \W  匹配非字母数字及下划线
	7. \s  匹配任意空白字符，等价于 [\t\n\r\f].
	8. \S  匹配任意非空字符
```

```
数量修饰符
	1.*    匹配0个或多个的表达式	
	2.+    匹配1个或多个的表达式
	3.?    匹配0个或1个由前面的正则表达式定义的片段
	4.{m}  前面字符出现m次
	5.{m,} 前面字符出现至少m次
	6.{m,n}前面字符出现m~n次
```

```
边界修饰符
	1.^    以...开始
	2.$    以...结尾
	eg:
      '^abc' 匹配以abc开头
      ‘abc$'  匹配以abc结尾
```

```
分组修饰符
	1.() 匹配括号内的表达式，也表示一个组	
	2.\1  \2  匹配第1、2个分组的内容
	eg:
     （.*）:(.*)
    	\1  \2
```

```
贪婪模式/非贪婪模式
    贪婪模式：在整个表达式匹配成功的前提下，尽可能多的匹配 ( * )；
    非贪婪模式：在整个表达式匹配成功的前提下，尽可能少的匹配 ( ? )；
    Python里数量词默认是贪婪的。
------------------------------------
示例一 ： 源字符串：abbbc
    使用贪婪的数量词的正则表达式 ab* ，匹配结果： abbb。 
    * 决定了尽可能多匹配 b，所以a后面所有的 b 都出现了。
    使用非贪婪的数量词的正则表达式ab*?，匹配结果： a。 
    即使前面有 *，但是 ? 决定了尽可能少匹配 b，所以没有 b。
          
示例二 ： 源字符串：aa<div>test1</div>bb<div>test2</div>cc
    使用贪婪的数量词的正则表达式：<div>.*</div>
    匹配结果：<div>test1</div>bb<div>test2</div>
    这里采用的是贪婪模式。在匹配到第一个“</div>”时已经可以使整个表达
    式匹配成功，但是由于采用的是贪婪模式，所以仍然要向右尝试匹配，
    查看是否还有更长的可以成功匹配的子串。匹配到第二个“</div>”后，
    向右再没有可以成功匹配的子串，匹配结束，匹配结果为
    “<div>test1</div>bb<div>test2</div>”

    使用非贪婪的数量词的正则表达式：<div>.*?</div>
    匹配结果：<div>test1</div>
    正则表达式二采用的是非贪婪模式，在匹配到第一个“</div>”
    时使整个表达式匹配成功，由于采用的是非贪婪模式，
    所以结束匹配，不再向右尝试，匹配结果为“<div>test1</div>”。
```

```
模式修饰符
	regular expression			
		1.re.S  单行模式
		2.re.M  多行模式
		3.re.I  忽略大小写
```

```
使用步骤：
      re.complie()
          compile 函数用于编译正则表达式，生成一个正则表达式Pattern对象
          Pattern对象 = re.compile(正则表达式)
          Pattern对象》find all（html）
      findall()
          在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。
```

```python
# 正则表达式-解析字符串
import re

content = '''
    <div class="thumb">
        <a href="/article/119749308" target="_blank">
        <img src="//pic.qiushibaike.com/system/pictures/11974/119749308/medium/app119749308.jpg" alt="糗事#119749308" class="illustration" width="100%" height="auto">
        </a>
    </div>
    <div class="thumb">
        <a href="/article/119750010" target="_blank">
        <img src="//pic.qiushibaike.com/system/pictures/11975/119750010/medium/app119750010.jpg" alt="糗事#119750010" class="illustration" width="100%" height="auto">
        </a>
    </div>
    <div class="thumb">
        <a href="/article/121859652" target="_blank">
        <img src="//pic.qiushibaike.com/system/pictures/12185/121859652/medium/LZB48T44DNIAY1CV.jpg" alt="糗事#121859652" class="illustration" width="100%" height="auto">
        </a>
    </div>
'''

# pattern = re.compile('pic.*?\.jpg')
pattern = re.compile('<div class="thumb">.*?<img src="(.*?)" alt=".*?"',re.S)
src_list = pattern.findall(content)
for src in src_list:
    print(src)
```

```python
# 正则表达式-站长素材
# 在控制台上输入起始页码  输入结束页码 然后下该页码中的所有的图片保存到qinglv的文件夹下
# http://sc.chinaz.com/tupian/qinglvtupian.html
# http://sc.chinaz.com/tupian/qinglvtupian_2.html
# http://sc.chinaz.com/tupian/qinglvtupian_3.html
import urllib.request
import re

def create_request(page):
    base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
    if page == 1:
        url = base_url + '.html'
    else:
        url = base_url + '_' + str(page) + '.html'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
    }

    request = urllib.request.Request(url=url,headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    # print(content)
    return content


def down_load(content):
    # pattern = re.compile('<div class="box picblock col3".*?<img src2="(.*?)" alt="(.*?)">',re.S)
    pattern = re.compile('<div class="box picblock col3".*?src2="(.*?)" alt="(.*?)"></a>',re.S)
    objs = pattern.findall(content)
    for obj in objs:
        suffix = obj[0].split('.')[-1]
        filename = './qinglv/'+obj[1]+'.'+suffix
        urllib.request.urlretrieve(url=obj[0],filename=filename)


if __name__ == '__main__':
    start_page =int(input('请输入起始页码'))
    end_page = int(input('请输入结束页码'))
    for page in range(start_page,end_page + 1):
        request = create_request(page)
        content = get_content(request)
        down_load(content)
```



## 二、xpath

```tex
xpath使用：
	注意：
	1.安装lxml库
		pip install lxml -i http://pypi.douban.com/simple
	2.导入lxml.etree
		from lxml import etree
	3.tree = etree.parse()解析本地文件
	  tree = etree.HTML(response.read().decode('utf-8'))解析服务器文件
	4.tree.xpath(xpath语法)
```

```tex
xpath基本语法：
1.路径查询
	//:查找所有的子孙节点，不考虑层级关系
	/ :找直接子节点（在开头位置表示根节点）
2.谓词查询
	//div[@id]/a[1] 下标从1开始 
			   a[last()] 最后一个a元素
			   string()函数 提取标签内所有文本
	//div[@id="maincontent"]
3.属性查询
	//@class
4.模糊查询
	//div[contains(@id,'he')]
	//div[starts-with(@id,'he')]
5.内容查询
	//div/h1/text()
6.逻辑运算
	//div[@id="head" and @class="s_down"]
	//title | //price
```

```python
# xpath可以解析本地文件
#   html_tree = etree.parse('XX.html')
# xpath可以解析服务器响应的文件  response.read().decode()
#   html_tree = etree.HTML(response.read().decode('utf-8')

#   html_tree.xpath(xpath语法)

from lxml import etree

# 本地文件中必须所有的标签都有结尾（单行标签内部末尾加/）
tree = etree.parse('index.html')
#       //：查找所有子孙节点，不考虑层级关系
# 		/ ：找直接子节点
# a = tree.xpath('//div[@id="d1"]//li')
# print(a)

# 找到div标签中有id属性的
# a = tree.xpath('//div[@id]')
# print(a)

# 属性查询
# a = tree.xpath('//div[@id="d1"]/ul/li[1]/@class')
# print(a)

# a_list = tree.xpath('//div[@class="d2"]/ul/li[contains(@id,"l")]/@id')
# for a in a_list:
#     print(a)

# a = tree.xpath('//div[@class="d2"]/ul/li[starts-with(@id,"c")]/@id')
# print(a)

a = tree.xpath('//div[@id="d1"]/a/text()')
print(a)
```



```python
# xpath-站长素材
import urllib.request


def create_request(page):
    base_url = 'http://sc.chinaz.com/tupian/qinglvtupian'
    if page == 1:
        url = base_url + '.html'
    else:
        url = base_url + '_' + str(page) + '.html'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'
    }
    request = urllib.request.Request(url=url,headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content

from lxml import etree

def down_load(content):
    tree = etree.HTML(content)
    src_list = tree.xpath('//div[@id="container"]//a/img/@src2')
    alt_list = tree.xpath('//div[@id="container"]//a/img/@alt')
    for i in range(len(src_list)):
        src = src_list[i]
        alt = alt_list[i]
        filename = './qinglv/'+alt + '.jpg'
        urllib.request.urlretrieve(url=src,filename=filename)


if __name__ == '__main__':
    start_page = int(input('请输入起始页码'))
    end_page = int(input('请输入结束页码'))

    for page in range(start_page,end_page+1):
        request = create_request(page)
        content = get_content(request)
        down_load(content)
```



## 三、JsonPath

```tex
jsonpath的安装及使用方式：
	pip安装：
		pip install jsonpath
	jsonpath的使用
		obj = json.load(open('json文件','r',encoding='utf-8'))
		res = jsonpath.jsonpath(obj,'json语法')
```

```tex
json对象的转换
	json.loads()
		是将json字符串转化为python对象
	json.dumps()
		将python对象转化为json格式
	json.load()
		读取json格式的文本，转化为python对象
		json.load(open(a.json))
	json.dump()
		将python对象写入到文本中
```

```python
import jsonpath
import json

obj = json.load(open('store.json','r',encoding='utf-8'))

# 书店所有书的作者
# author_list = jsonpath.jsonpath(obj,'$.store.book[*].author')
# for author in author_list:
#     print(author)


# 所有的作者
# author_list = jsonpath.jsonpath(obj,'$..author')
# for author in author_list:
#     print(author)

# store的所有元素。所有的books和bicycle
# a_list = jsonpath.jsonpath(obj,'$.store.*')

# store里面所有东西的price
# price_list = jsonpath.jsonpath(obj,'$.store..price')
# # for price in price_list:
# #     print(price)

#
# 第三个书
# book = jsonpath.jsonpath(obj,'$..book[2]')
# print(book)

# 最后一本书
# book = jsonpath.jsonpath(obj,'$..book[(@.length-1)]')
# print(book)

# 	前面的两本书。
# book_list = jsonpath.jsonpath(obj,'$..book[0,1]')
# print(book_list)
# book_list = jsonpath.jsonpath(obj,'$..book[:2]')
# print(book_list)

# 过滤出所有的包含isbn的书。
# book_list = jsonpath.jsonpath(obj,'$..book[?(@.isbn)]')
# print(book_list)

# 	过滤出价格低于10的书
# book_list = jsonpath.jsonpath(obj,'$..book[?(@.price < 10)]')
# print(book_list)
```

教程连接（http://blog.csdn.net/luxideyao/article/details/77802389）

```python
# jsonpath-智联招聘
import jsonpath
import json
import urllib.request

url = 'https://fe-api.zhaopin.com/c/i/sou?pageSize=90&cityId=538&workExperience=-1&education=-1&companyType=-1&employmentType=-1&jobWelfareTag=-1&kw=python&kt=3&_v=0.45173956&x-zp-page-request-id=8e8e88b85c494aac8ab3533ebc6ddd5a-1571908246825-8289&x-zp-client-id=94df40c0-69a5-40f2-9d66-0bbe28f523a4'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36',
}
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
with open('zhilian.json','w',encoding='utf-8')as fp:
    fp.write(content)

obj = json.load(open('zhilian.json','r',encoding='utf-8'))

jobname_list = jsonpath.jsonpath(obj,'$..jobName')
companyname_list = jsonpath.jsonpath(obj,'$..company.name')
salary_list = jsonpath.jsonpath(obj,'$..salary')

jobInfo_list = []

for i in range(len(jobname_list)):
    job = jobname_list[i]
    cname = companyname_list[i]
    salary = salary_list[i]
    jobInfo = {}
    jobInfo['job']=job
    jobInfo['cname']=cname
    jobInfo['salary']=salary
    jobInfo_list.append(jobInfo)

with open('zhilian1.json','w',encoding='utf-8')as fp:
    fp.write(str(jobInfo_list))
```

应用案例：智联招聘（薪水，公司名称，职位需求）

作业： 1.股票信息提取（http://quote.stockstar.com/）

​	         2.boos直聘

​             3.中华英才

​             4.汽车之家

## 四、BeautifulSoup

1.基本简介

```
1.BeautifulSoup简称：
	bs4
2.什么是BeatifulSoup？
	BeautifulSoup，和lxml一样，是一个html的解析器，主要功能也是解析和提取数据
3.优缺点？
	缺点：效率没有lxml的效率高	
	优点：接口设计人性化，使用方便
```

2.安装以及创建

```
1.安装
	pip install bs4
2.导入
	from bs4 import BeautifulSoup
3.创建对象
	服务器响应的文件生成对象
		soup = BeautifulSoup(response.read().decode(), 'lxml')
	本地文件生成对象
		soup = BeautifulSoup(open('1.html'), 'lxml')
		注意：默认打开文件的编码格式gbk所以需要指定打开编码格式
```

3.节点定位

```
1.根据标签名查找节点
    soup.a 【注】只能找到第一个a
        soup.a.name
        soup.a.attrs
2.函数
(1).find(返回一个对象)
    find('a')：只找到第一个a标签
    find('a', title='名字')
    find('a', class_='名字')
(2).find_all(返回一个列表)
    find_all('a')  查找到所有的a
    find_all(['a', 'span'])  返回所有的a和span
    find_all('a', limit=2)  只找前两个a
(3).select(根据选择器得到节点对象)【推荐】
    1 element
    	eg:p
    2 .class
    	eg:.firstname
    3 #id
    	eg:#firstname
    4 属性选择器
        [attribute]
            eg:li = soup.select('li[class]')
        [attribute=value]
            eg:li = soup.select('li[class="hengheng1"]')
    5 层级选择器
        element element后代标签
            div p
        element>element直接子标签
            div>p
        element,element同级标签
            div,p 
        eg:soup = soup.select('a,span')
3.获取子孙节点
		contents：返回的是一个列表
			eg:print(soup.body.contents)
		descendants：返回的是一个生成器
			eg:for a in soup.body.descendants:
					print(a)
```

4.节点信息

```
(1).获取节点内容：适用于标签中嵌套标签的结构
    obj.string
    obj.get_text()【推荐】
(2).节点的属性
    tag.name 获取标签名
    eg:
    	tag = find('li)
    	print(tag.name)
tag.attrs将属性值作为一个字典返回
(3).获取节点属性
    obj.attrs.get('title')【常用】
    obj.get('title')
    obj['title']
```

5.节点类型

```
节点类型
	bs4.BeautifulSoup 根节点类型
	bs4.element.NavigableString 连接类型  可执行的字符串
	bs4.element.Tag 节点类型  
	bs4.element.Comment 注释类型
    eg:
    if type(aobj.string) == bs4.element.Comment:
    	print('这个是注释内容')
    else:
   		print('这不是注释')
```

应用实例： 1.股票信息提取（http://quote.stockstar.com/）

​		    		2.中华英才网-旧版

​                    3 .腾讯公司招聘需求抓

```python
from bs4 import BeautifulSoup

# 加载的是本地文件
# bs4默认打开文件的编码是gbk
soup = BeautifulSoup(open('index.html',encoding='utf-8'),'lxml')

# print(soup.a)
# =========================
# find 方法返回的是一个对象
# 查找到第一个a标签
# a = soup.find('a')
# print(a)
# 根据标签的属性来查找标签
# a = soup.find('a',title='qf')
# print(a)
# 不可以实现
# a = soup.find('a',name='fq')
# print(a)
# a = soup.find('a',id='qf1')
# print(a)

# a = soup.find('a',class_='bd')
# print(a)

# a = soup.find('a',bd='bd1')
# print(a)
# ================================
# findall
# a_list = soup.find_all('a')
# print(a_list)

# a_list = soup.find_all(['a','span'])
# print(a_list)

# li_list = soup.find_all('li',limit=2)
# print(li_list)
# ==================================
# select
# a = soup.select('a')
# print(a)
# li = soup.select('.l1')
# print(li)
# li = soup.select('#l2')
# print(li)
# li = soup.select('li[class]')
# print(li)
# li = soup.select('li[class="l1"]')
# print(li)
# li_list = soup.select('div li')
# print(li_list)
# li = soup.select('span > span')
# print(li)
# list = soup.select('a,span')
# print(list)
# ==========================
# 获取某标签下的所有的子节点
# print(soup.body.contents)
# for a in soup.body.descendants:
#     print(a)
# ============================
# 获取节点内容
# s = soup.select('#s1')[0]
# print(type(s))
# <span id="s1">
#         <span>还有一周我们爬虫就结束了</span>
# </span>
# string想要获取 那么该对象的标签内 不允许在存在标签
# print(s.string)
# print(s.get_text())

# name属性获取的是标签的名字
# s = soup.select('#s1')[0]
# print(s.name)

# 获取节点的属性
p = soup.select('#p1')[0]
print(p.attrs)
print(p.attrs.get('class'))
print(p.get('name'))
print(p['id'])
```

```python
# 股票网站的爬取
import urllib.request

url = 'http://quote.stockstar.com/'
headers = {
        'User - Agent': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 77.0.3865.120Safari / 537.36'
    }
request = urllib.request.Request(url=url,headers=headers)

response = urllib.request.urlopen(request)
content = response.read().decode('gb2312')

# 解析
from bs4 import BeautifulSoup

soup = BeautifulSoup(content,'lxml')

tr_list = soup.select('#datalist tr')
for tr in tr_list:
    code = tr.find_all('td')[0].get_text()
    name = tr.find_all('td')[1].get_text()
    price = tr.find_all('td')[2].get_text()
    print(code,name,price)
```

