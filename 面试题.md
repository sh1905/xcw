

### 单例模式

- 某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例，这个类称为单例类，单例模式是一种对象创建型模式
- 应用场景：
  - 资源共享的情况下，避免由于资源操作时导致的性能或损耗等
  - 控制资源的情况下，方便资源之间的互相通信

#### 创建单例-保证只有1个对象

```python
class Trash(object):
    __instance = None
    
    def __new__(cls,*args,**kwargs):
        # #如果类属性__instance的值为None，
        #那么就创建一个对象，并且赋值为这个对象的引用，保证下次调用这个方法时
        #能够知道之前已经创建过对象了，这样就保证了只有1个对象
        if cls.__instance is None:
            cls.__instance = object.__new__(cls)
         return cls.__instance
    
a= Trash()    
b= Trash()

# 同一内存地址
print(id(b))
print(id(a))

a.age = 19
# 19
print(b.age)
```

#### 创建单例时，只执行1次**init**方法

```python
# 实例化一个单例
class Singleton(object):
    __instance = None
    __is_first = True

    def __new__(cls, age, name):
        if not cls.__instance:
            cls.__instance = object.__new__(cls)
        return cls.__instance

    def __init__(self, age, name):
        if self. __is_first: # 不会再创建第二个对象
            self.age = age
            self.name = name
            Singleton. __is_first = False


a = Singleton(18, "张三")
b = Singleton(28, "张三")

print(id(a))
print(id(b))

# 第二个对象创建失败
print(a.age) # 18
print(b.age) # 18

a.age = 19
print(b.age)
```

#### 装饰器实现单例

```python
def singleton(cls, *args, **kw):
    instance={}
    def _singleton():
        if cls not in instance:
            instance[cls]=cls(*args, **kw)
        return instance[cls]
    return _singleton
 
@singleton
class test_singleton(object):
    def __init__(self):
        self.num_sum=0
    def add(self):
        self.num_sum=100    

if __name__ == '__main__':
    c1 = test_singleton()
    c2 = test_singleton()
    print(c1)
```

#### 说了下redis的5个数据类型

**String**:

String是最常用的一种数据类型，普通的key/ value 存储都可以归为此类.即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受Redis的定时持久化，操作日志及 Replication等功能。

**hash**

这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。

**List**

使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。本人还用一个场景，很合适---取行情信息。就也是个生产者和消费者的场景。LIST可以很好的完成排队，先进先出的原则

**Set**

因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。
 另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能

**Zset**

Redis zset的使用场景与set类似，区别是set不是自动有序的，而zset可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择zset数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的

### redis的持久化

redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时 dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）

+ RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。

+ AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录

 

### 怎么实现高并发这些

1. 页面静态化- 用户可以直接获取页面，不用走那么多流程，比较适用于页面不频繁更新。

2. 使用缓存- 第一次获取数据从数据库准提取，然后保存在缓存中，以后就可以直接从缓存提取数据。不过需要有机制维持缓存和数据库的一致性。

3. 使用储存过程-那些处理一次请求需要多次访问数据库的操作，可以把操作整合到储存过程，这样只要一次数据库访问就可以了。

4. 批量读取 - 高并发情况下，可以把多个请求的查询合并到一次进行，以减少数据库的访问次数

5. 延迟修改 - 高并发情况下，可以把多次修改请求，先保存在缓存中，然后定时将缓存中的数据保存到数据库中，风险是可能会断电丢失缓存中的数据，

6. 使用索引 - 索引可以看作是特殊的缓存，尽量使用索引就要求where字句中精确的给出索引列的值。



### 网络编程相关 http相关 



### orm模型优缺点

ORM的全称是：Object Relational Mapping （对象 关系 映射）

简单的说，orm是通过使用描述对象和数据之间映射的元数据，将程序中的对象自动持久化到关系数据库中。

ORM需要解决的问题是，能否把对象的数据直接保存到数据库中，又能否直接从数据库中拿到一个对象？要想做到上面两点，则必须要有映射关系。

+ 优点：

  orm的技术特点，提高了开发效率。可以自动对实体Entity对象与数据库中的Table进行字段与属性的映射；不用直接SQL编码，能够像操作对象一样从数据库中获取数据

+ 缺点：

  orm会牺牲程序的执行效率和会固定思维模式，在从系统结构上来看，采用orm的系统多是多层系统的，系统的层次太多，效率就会降低，orm是一种完全面向对象的做法，所以面向对象的做法也会对性能产生一定的影响。

### 非关系型和关系型数据库区别

+ SQL数据存在特定结构的表中;而NoSQL比较灵活并具有可扩展性,存储方式可以是JSON文档,哈希表或其他方式.

+ 在SQL中,必须要先定义好表和字段结构后才能添加数据,例如定义表的主键,索引,触发器,存储过程等.表结构可以在被定义之后更新,但是如果有比较大的结构变更的话就会变得比较复杂.在NoSQL中,数据可以在任何时候任何地方添加,不需要先定义表.

+ SQL中如果需要增加外部关联数据的话,规范化做法就是在原表中增加一个外键,关联外部数据表.而在NoSQL中除了这种规范化的外部数据表做法外,我们还能用如下的非规范方式把外部数据直接放到原数据中,以提升查询效率.但是缺点也比较明显,更新审核人数据的时候将会比较麻烦.

+ SQL中可以使用JOIN表连接方式将多个关系表中数据用一条简单的查询语句查询出来.NoSQL暂时还没有提供类似JOIN的查询方式对多个数据集的数据做查询.这也是大部分NoSQL使用非规范化的数据存储方式存储数据的原因

+ SQL中不允许删除已经被使用的外部数据,而NoSQL中则没有这种强耦合的概念,可以随时删除任何数据.

+ SQL中如果多张表数据需要同批次被更新,即如果其中一张表更新失败的话其他表也不能更新成功.这种场景可以通过事物来控制,可以在所以命令完成后在统一提交事务.而NoSQL中没有事物这一概念,每一个数据集的操作都是原子级的.

+ 在相同水平的系统设计的前提下,因为NoSQL中省略了JOIN查询的消耗,所以理论上在性能上是优于SQL的.

### 分库分表

```
在cache层的高速缓存，MySQL的主从复制，读写分离的基础上，这时MySQL主库的写压力开始出现瓶颈，而数据量的持续猛增，由于MyISAM使用表锁，在高并发下会出现严重的锁问题，大量的高并发MySQL应用开始使用InnoDB引擎代替MyISAM。采用Master-Slave复制模式的MySQL架构，只能对数据库的读进行扩展，而对数据的写操作还是集中在Master上。这时需要对数据库的吞吐能力进一步地扩展，以满足高并发访问与海量数据存储的需求。

对于访问极为频繁且数据量巨大的单表来说，首先要做的是减少单表的记录条数，以便减少数据查询所需的时间，提高数据库的吞吐，这就是所谓的分表。在分表之前，首先需要选择适当的分表策略，使得数据能够较为均衡地分布到多张表中，并且不影响正常的查询。

　　分表能够解决单表数据量过大带来的查询效率下降的问题，但是却无法给数据库的并发处理能力带来质的提升。面对高并发的读写访问，当数据库master服务器无法承载写操作压力时，不管如何扩展Slave服务器都是没有意义的，对数据库进行拆分，从而提高数据库写入能力，即分库。

　　数据库经过业务拆分及分库分表，虽然查询性能和并发处理能力提高了。但是原本跨表的事务上升为分布式事务；由于记录被切分到不同的库和不同的表中，难以进行多表关联查询，并且不能不指定路由字段对数据进行查询。且分库分表后需要进一步对系统进行扩容（路由策略变更）将变得非常不方便，需要重新进行数据迁移。
```

+ 分表策略：

　　

```
使用用户ID是最常用的分库的路由策略。

　　当数据比较大的时候，对数据进行分表操作，首先要确定需要将数据平均分配到多少张表中，也就是：表容量。

　　这里假设有100张表进行存储，则我们在进行存储数据的时候，首先对用户ID进行取模操作，根据 user_id%100 获取对应的表进行存储查询操作。

　　在实际的开发中，我们的用户ID更多的可能是通过UUID生成的，这样的话，我们可以首先将UUID进行hash获取到整数值，然后在进行取模操作。
```

+  分库策略：

 

```
 数据库分表能够解决单表数据量很大的时候数据查询的效率问题，但是无法给数据库的并发操作带来效率上的提高，因为分表的实质还是在一个数据库上进行的操作，很容易受数据库IO性能的限制。

　　因此，如何将数据库IO性能的问题平均分配出来，很显然将数据进行分库操作可以很好地解决单台数据库的性能问题。

　　分库策略与分表策略的实现很相似，最简单的都是可以通过取模的方式进行路由。
```

 

+ 分库与分表实现策略：　　

```
上述的配置中，数据库分表可以解决单表海量数据的查询性能问题，分库可以解决单台数据库的并发访问压力问题。

　　有时候，我们需要同时考虑这两个问题，因此，我们既需要对单表进行分表操作，还需要进行分库操作，以便同时扩展系统的并发处理能力和提升单表的查询性能，就是我们使用到的分库分表。

　　分库分表的策略相对于前边两种复杂一些，一种常见的路由策略如下：

　　１、中间变量　＝ user_id%（库数量*每个库的表数量）; 　　

　　２、库序号　＝　取整（中间变量／每个库的表数量）; 　　

　　３、表序号　＝　中间变量％每个库的表数量;
```



### 数据库集群

顾名思义，就是利用至少两台或者多台数据库服务器，构成一个虚拟单一数据库逻辑映像，像单数据库系统那样，向客户端提供透明的数据服务

这里有两个关键点：

+ 两台或者多台数据库服务器：如果只有一台数据库服务器是不能称其为集群的。

+ 透明的服务：集群向客户端提供的服务与单机系统向客户端提供的服务，从通讯协议上保持二进制兼容。

为什么要用数据库集群

通过使用数据库集群可以使读写分离，提高数据库的系统性能。

​    大家都知道，mysql是支持分布式的。MySQL Proxy最强大的一项功能是实现“读写分离(Read/Write 

Splitting)”。基本的原理是让主数据库处理事务性查询，而从数据库处理SELECT查询。数据库复制被

用来把事务性查询导致的变更同步到集群中的从数据库，从而使从数据库和主数据库的数据保持一致。 

当然，主服务器也可以提供查询服务。使用读写分离最大的作用无非是环境服务器压力

### 主从复制 

MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默

认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连

接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。

MySQL 主从复制主要用途

+ 读写分离

  在开发工作中，有时候会遇见某个sql 语句需要锁表，导致暂时不能使用读的服务，这样就会影响现

  有业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。

+ 数据实时备份，当系统中某个节点发生故障时，可以方便的故障切换

+ 高可用HA

+ 架构扩展

  随着系统中业务访问量的增大，如果是单机部署数据库，就会导致I/O访问频率过高。有了主从复

  制，增加多个数据存储节点，将负载分布在多个从节点上，降低单机磁盘I/O访问的频率，提高单个

  机器的I/O性能。

### Nginx相关 

### 轮询

ajax轮询 ，ajax轮询 的原理非常简单，让浏览器隔个几秒就发送一次请求，询问服务器是否有新信息。

长轮询

ong poll 其实原理跟 ajax轮询 差不多，都是采用轮询的方式，不过采取的是阻塞模型（一直打电话，没

收到就不挂电话），也就是说，客户端发起连接后，如果没消息，就一直不返回Response给客户端。直

到有消     息才返回或超时，返回完之后，客户端再次建立连接，周而复始，基于事件的触发，一个事件

接一个事件。 

Ajax轮询与long poll都属于不断发送http请求，然后等待服务器处理，可以看到http协议一个特点，被

动性，服务端不能主动联系客户端，只有客户端发起。

缺点：Ajax轮询需要服务器有很快的处理速度与快速响应。long poll需要很高的并发，体现在同时容纳

请求的能力。

### 最小权重 



### ip 



### hash 的方法原理



### zset差集

redis集合(set)计算差集的命令是 sdiff。那么有序集合(zset)的差集是用 zdiff 命令吗？redis api目前是不支持这个命令的

使用场景例子

有序集合记录了英语兴趣课(key=english)、数学兴趣课(key=math)的学员及成绩。



```sql
127.0.0.1:6379> zadd english 98 user1 85 user2 88 user3

(integer) 3

127.0.0.1:6379> zadd math 77 user2 69 user4 100 user5

(integer) 3
```

需要计算出只参加了英语兴趣课的学员



解决思路

A）使用集合diff

客户端程序把有序集合的数据导入到无序集合，通过集合的diff计算出结果

B）利用有序集合里已有命令

\* ZUNIONSTORE 计算并集，并设置并集后元素计分权重

\> ZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]

\* ZREMRANGEBYSCORE 去除计分为0的元素

\> ZREMRANGEBYSCORE key min max

### redis命令实现

```sql
127.0.0.1:6379> zunionstore result 2 english math weights 1 0 aggregate min

(integer) 5

127.0.0.1:6379> zremrangebyscore result 0 0

(integer) 3

127.0.0.1:6379> zrange result 0 -1

1) "user3"

2) "user1"
```

#### python的第三方库

- re    匹配正则    re.findall
- requests     用来发送求    requests.put
- os    os.remove()  删除一个文件
- time        time time() 返回当前时间的时间戳（1970纪元后经过的浮点秒数）



#### 1.项目中的权限管理怎么做的，项目中第三方登录的实现过程



#### 2.介绍flask中蓝图和请求钩子

- 为了解决循环导入问题，就是代替app统一管理路由



- 钩子函数

  - **第一个钩子：@app.before_first_request**

    只在第一次请求之前执行，也就是启动项目，不会执行，只会在第一次有人发起请求时，才会触发这个钩子中的代码。

    全局场景:可以带动一个异步执行的函数，进行一些健康指标的检查，如果发现有异常，则截断后续的请求，将整个Flask应用停止。

  - **第二个钩子：@app.before_request**

    这是最重要的一个钩子，在每次请求之前可以注入你要的逻辑的钩子。在app下的before_request，过滤的是全部请求。结合Blueprint的before_request，则是过滤该蓝图下的请求。所以我们就可以进行分层过滤，定制化过滤。

    全局的场景包含：共享session的鉴权函数、请求黑白名单过滤、根据endpoint进行请求j等。

    蓝图场景包含api的请求必填字段校验，是否json请求校验，请求的token校验等。

  - **第三个钩子：@app.errorhandler**

    当访问应用出错时，根据错误响应码，进行一些定制化的操作，如返回一个可爱的404页面。也可以进行一些报错登记。

    场景：可以用redis进行错误请求计数，超过一定量则进行告警。可以重定向到一个定制的错误代码页面等。

  - **第四个钩子：@app.context_processor**

    这个钩子也很实用，是将一些常量按字典的格式返回，则可以在jinja2的模版中引用。这样就不用在每个视图函数中都render_template中重复去写一次。代码更简洁。

    场景：在html中，直接用{{jidan}}就会在页面显示yao。等同于app.add_template_global('yao',''jidan)

  - **第五个钩子：@app.after_request**

    和上个钩子类似，差别在于是请求完成时执行，它和之前钩子有点不同，必须传入一个参数来接收响应对象，并在最后return 这个参数，也就是返回响应内容。

    场景：一般用于格式化响应结果，包括响应请求头，响应的格式等。

  - **第六个钩子：@app.teardown_request**

    和第五个钩子功能类似，在响应销毁时，执行一个绑定的函数。做一些操作。

    区别点在于：

     ●  after_request: 每一个请求之后绑定一个函数，如果请求没有异常。

     ●  teardown_request: 每一个请求之后绑定一个函数，即使遇到了异常。

    场景：销毁DB连接等。

  - **第七个钩子：@app.teardown_appcontext**

    之前介绍的大部分是请求上下文的钩子，这个属于应用上下文的钩子。不管是否有异常，当APP上下文被移除之后执行的函数, 可以进行数据库的提交或者回滚。

    场景：DB事务操作。

#### 3.python几种数据结构，元组列表区别

- 列表、元组、集合、字符串、整型、浮点数、复数、bool、字典
- 元组一旦改变不可更改、列表可以改变其元素

#### 4.外卖取餐场景，怎么实现取餐信息推送提醒功能，定时任务的几种模式

```shell
1.定时任务代码

#!/user/bin/env python
# @Time   :2018/6/7 16:31
# @Author  :PGIDYSQ
#@File   :PerformTaskTimer.py
#定时执行任务命令
import time,os,sched
schedule = sched.scheduler(time.time,time.sleep)
def perform_command(cmd,inc):
  os.system(cmd)
  print('task')
def timming_exe(cmd,inc=60):
  schedule.enter(inc,0,perform_command,(cmd,inc))
  schedule.run()
print('show time after 2 seconds:')
timming_exe('echo %time%',2)

2.周期性执行任务
#!/user/bin/env python
# @Time   :2018/6/7 16:31
# @Author  :PGIDYSQ
#@File   :PerformTaskTimer.py
import time,os,sched
schedule = sched.scheduler(time.time,time.sleep)
def perform_command(cmd,inc):
  #在inc秒后再次运行自己，即周期运行
  schedule.enter(inc, 0, perform_command, (cmd, inc))
  os.system(cmd)
def timming_exe(cmd,inc=60):
  schedule.enter(inc,0,perform_command,(cmd,inc))
  schedule.run()#持续运行，直到计划时间队列变成空为止
print('show time after 2 seconds:')
timming_exe('echo %time%',2)

3.循环执行命令
#!/user/bin/env python
# @Time   :2018/6/7 16:31
# @Author  :PGIDYSQ
#@File   :PerformTaskTimer.py
import time,os
def re_exe(cmd,inc = 60):
  while True:
    os.system(cmd)
    time.sleep(inc)
re_exe("echo %time%",5)
```



#### 5.左关联和右关联查询，口诉一些sql查询语句。

+ 左连接：LEFT JOIN
  + left join 关键字从左表（table1）返回所有的行，即使右表（table2）中没有匹配。如果右表中没有匹配，则结果为NULL
  + 语法：select 字段名 from 表1 left join 表2 on 表1.字段 = 表2.字段;
+ 右连接：RIGHT JOIN(与左连接结果相反)
  + right join关键字从右表（table2）返回所有的行，即使左（table1）中没有匹配。如果左表中没有匹配，则结果为NULL
  + 语法：select 字段名 from 表1 right join 表2 on 表1.字段 = 表2.字段;

#### 6.联合索引的最左原则查询，用abc举例

- 在mysql建立联合索引时会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配

  只能a、ab 、abc 查询 ，不可以ac bc 查询

#### 7.https1.0 2.0区别 http

- 1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。
- 2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。
- 3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
- 4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。

#### 8.在三次握手中哪个环节容易受到黑客攻击  三次握手四次挥手

```shell
浏览器与服务器的通信

浏览器访问：http://www.obcde.com/x/t/z.html

1.DNS解析：www.abcde.com  ->>  DNS  服务器    -->  11.22.33.44

2.浏览器与服务器建立TCP连接

	#三次握手（用来建立连接）

					TCP   报文   （是二进制的数据流）

	1）client    -->    SYN    -->    server     SYN:跟服务器一个请求，代表想跟服务器询问想建立一个链接，请求服务器的同意(报文)
#seq是数据包本身的序列号；ack是期望对方继续发送的那个数据包的序列号。
	2)   client   <--   ACK  +    SYN   <--  server    

	3)   client   -->    ACK 	--> 		server

3.封装    HTTP   request 报文（是一对人能识别的字符串）

4.将报文发送给服务器

5.服务器接收，并解析

6.服务器做出相应处理（逻辑运算、数据存取、数据修改）

7.服务器封装    HTTP Response    报文

8.服务器将报文发送给  浏览器

9.服务器断开与客户端的连接

	#四次挥手
	1.server  -> FIN->   client    FIN(二进制的结束报文)    finally
	2.server  <- ACK <- client     ACK   客户端确认（知道了的接受结束）的报文传给服务器
	3.server  <- FIN <-   client    客户端要验证传输的数据是否全部正确和是否所有数据的全部接受完毕
	4.server  ->  ACK  ->  client  服务器断开连接
10.浏览器接受响应报文

11.解析、呈现
	eg：当服务器接受客户端的请求，服务器会在内存中单独开辟一个空间，所以服务器要接受完浏览器的三次挥手  request处理完毕    返回响应的时候就会断开连接

	HTTP    建立在    TCP    基础之上的短连接协议

	TCP    全双工    可以互相的交互    不用协定时间，可以统一时间进行交互
	半双工    ：是相互的协议，1时间段是服务器向客户端发送数据，2时间客户端向服务器发送数据
```

##### 1.三次握手

三次握手建立连接阐述：
第一次握手：客户端要和服务端进行通信，首先要告知服务端一声，遂发出一个SYN=1的连接请求信号,”服务端哥哥，我想给你说说话”。

第二次握手：当服务端接收到客户端的连接请求，此时要给客户端一个确认信息，”我知道了（ACK）,我这边已经准备好了，你现在能连吗（SYN）”。

第三次握手：当客户端收到了服务端的确认连接信息后，要礼貌的告知一下服务端，“好的，咱们开始联通吧（ACK）”。

到此整个建立连接的过程已经结束，接下来就是双方你一句我一句甚至同时交流传递信息的过程了。



##### 2.四次挥手

四次挥手断开连接阐述：
第一次挥手：双方交流的差不多了，此时客户端也已经结尾了，接下来要断开通信连接，所以告诉服务端“我说完了（FIN）”，此时自身形成等待结束连接的状态。

第二次挥手：服务端知道客户端已经没话说了，服务端此时还有两句心里话要给客户端说，“我知道你说完了（ACK），我再给你说两句，&*……%￥”。

第三次挥手：此时客户端洗耳恭听继续处于等待结束的状态，服务器端也说完了，自身此时处于等待关闭连接的状态，并对告诉客户端，“我说完了，咱们断了吧（FIN）”。

第四次挥手：客户端收知道服务端也说完了，也要告诉服务端一声（ACK），因为连接和断开要双方都按下关闭操作才能断开，客户端同时又为自己定义一个定时器，因为不知道刚才说的这句话能不能准确到达服务端（网络不稳定或者其他因素引起的网络原因），默认时间定为两个通信的最大时间之和，超出这个时间就默认服务器端已经接收到了自己的确认信息，此时客户端就关闭自身连接，服务器端一旦接收到客户端发来的确定通知就立刻关闭服务器端的连接。

**到此为止双方整个通信过程就此终结。这里要声明一下：断开链接不一定就是客户端，谁都可以先发起断开指令，另外客户端和服务端是没有固定标准的，谁先发起请求谁就是客户端。**

#### 9.三大范式 

- 第一范式:确保每一列原子化(不可分割)

- 第二范式:,基于第一范式,一张表只能描述一件事情,非主键字段必须依赖主键字段(不论在什么情况下主键都是唯一的)

- 第三范式:基于第二范式,消除传递依赖(一个主键字段可以确定其它的信息)

  eg:一个表中不要重复记录别的表的信息，也就是当一张表的关联表能找到这个信息，那么就不要在把关联表的信息在写道这张表中。

#### 10.事务 特性

- 什么是mysql中的事务?
  - 1.事务是一个不可拆分的工作单元
  - 2.事务作为一个整体向系统提交,要么一起执行成功,要么一起失败
  - 3.事务不支持嵌套
- 事物的特性
  - 1.原子性:不可被拆分
  - 2.一致性:要么一起执行成功,要么一起失败
  - 3.隔离性:事务彼此之间没有关系
  - 4.永久性:一旦执行成功,不可被修改

#### 11.栈和堆

- 栈   后进先出    存储的是id 和变量名
- 堆   先进后出    存储的是id  数值类型    值

#### 12.列表和字符串的相互转换

```shell
1. 字符串转列表
str1 = "hi hello world"
print(str1.split(" "))
输出：
['hi', 'hello', 'world']
 

2. 列表转字符串
l = ["hi","hello","world"]
print(" ".join(l))
输出：
hi hello world
```

#### 13.with open  Python之上下文管理器

```shell
file = open('test.txt')
for line in file:
    print(line)
file.close()
```

- 协议指出，创建的类必须至少定义两个方法: `__enter__`和`__exit__`。这就是协议，如果类遵循这个协议，就可以挂接到with语句。

#### 14.开发环境、测试环境、生产环境的区别

开发环境：开发环境是程序猿们专门用于开发的服务器，配置可以比较随意， 为了开发调试方便，一般打开全部错误报告。

测试环境：一般是克隆一份生产环境的配置，一个程序在测试环境工作不正常，那么肯定不能把它发布到生产机上。

生产环境：是指正式提供对外服务的，一般会关掉错误报告，打开错误日志。

三个环境也可以说是系统开发的三个阶段：开发->测试->上线，其中生产环境也就是通常说的真实环境。

#### 15.django生命周期

1. 当用户在浏览器中输入url时,浏览器会生成请求头和请求体发给服务端
   请求头和请求体中会包含浏览器的动作(action),这个动作通常为get或者post,体现在url之中.

2. url经过Django中的wsgi,再经过Django的中间件,最后url到过路由映射表,在路由中一条一条进行匹配,
   一旦其中一条匹配成功就执行对应的视图函数,后面的路由就不再继续匹配了.
3. 视图函数根据客户端的请求查询相应的数据.返回给Django,然后Django把客户端想要的数据做为一个字符串返回给客户端.
4. 客户端浏览器接收到返回的数据,经过渲染后显示给用户.

#### 16.栈



- 栈  有序  先进后出  后进先出  内存小  存储的是变量

- 堆  无序  内存大  存储的是对象

- 栈的执行效率快、内存低

- 对的执行效率低、内存大

- 详解栈和堆的对比请看笔记X-mind“堆栈”文件

- 注：

  什么进什么出，一共有几种？两种

  先进先出--------》队列-----------》后进后出

  先进后出--------》栈---------------》后进先出

  栈可以想象成一个桶状物。队列可以想象成一个长条状物，右边进，左边出

  https://www.jianshu.com/p/05cc2ffe66f3

栈的示例代码

```shell
#压栈
mystack = []
mystack.append(1)
print(mystack)

#出栈
mystack.pop()
print(mystack)
```

#### 17.序列化反序列化

```sehll
pickle模块的使用
对于大多数应用程序来讲，dump()和load()函数的使用就是你使用pickle 
模块所需的全部了。 
dumps(object)和dump(object) :序列化 
loads(bytes)和load(bytes):反序列化 
两者不同的是不带s的是(反)序列化关于二进制文件中,带s是(反)关于序列化对象

import pickle
dict1 = {"a": 1, "b": 2, "c": 3} 
b = pickle.dumps(dict1)
print(b,type(b)) # 1
c = pickle.loads(b)
print(c,type(c)) # 2

输出结果:
1:b'\x80\x03}q\x00(X\x01\x00\x00\x00aq\x01K\x01X\x01\x00\x00\x00bq\x02K\x02X\x01\x00\x00\x00cq\x03K\x03u.' <class 'bytes'>
2:{'a': 1, 'b': 2, 'c': 3} <class 'dict'>

pickle.dump和pickle.load的使用:序列和反序列化到文件中的

import pickle
dict1 = {"a": 1, "b": 2, "c": 3} 
sfile = open("dump.txt", "wb")
pickle.dump(dict1, sfile)# 
sfile.close()
dfile = open("dump.txt", "rb")
result = pickle.load(dfile)
dfile.close()
print(id(dict1), result, id(result)) # 1
输出结果:
1:1433748979648 {'a': 1, 'b': 2, 'c': 3} 1433751817024
结果说明反序列化后的对象不是原来的对象了

```

django代替django自带的runserver服务器

- 用guncore+gevent    这个guncore加一个配置文件，性能不够

Gunicorn + Gevent（meinheld）
 Gunicorn就是代替django中的runserver
 Gunicorn就是 HTTP Server的角色
 django就是web App 的角色
 WSGI就是连接Gunicorn和django的

 Gunicorn配置 gunicorn-config.py

```python
# -*- coding: utf-8 -*-

from multiprocessing import cpu_count

bind = ["127.0.0.1:9000"]  # 线上环境不会开启在公网 IP 下，一般使用内网 IP
daemon = True  # 是否开启守护进程模式
pidfile = 'logs/gunicorn.pid' # 记录的使主进程的ID，会直接生成这个文件

workers = 8 #cpu_count() * 2  # 工作进程数量
# worker_class = "gevent"  # 指定一个异步处理的库
worker_class = "egg:meinheld#gunicorn_worker"  # 比 gevent 更快的一个异步网络库
worker_connections = 65535  # 单个进程的最大连接数

keepalive = 60  # 服务器保持连接的时间，能够避免频繁的三次握手过程
timeout = 30
graceful_timeout = 10
forwarded_allow_ips = '*'

# 日志处理
capture_output = True
loglevel = 'info'
errorlog = 'logs/error.log'
```

#### 18.第一阶段笔试题

```shell
a=[1,2,3,None,(),[]]
print(len(a))
x=[(a,a+1) for a in range(5)]
y=dict((a,b) for a,b in x)
print(y)
list=['Tech',404,3.03,'Beamers',33.3]
print(list[1:3])
def f(x,l=[]):
    for i in range(x):
        l.append(i*i)
        print(l)
a=[1,9,1,2]
f(2,a)
f(3,a)

```

#### 19.事务

```mysql
什么是mysql中的事务?
1.事务是一个不可拆分的工作单元
2.事务作为一个整体向系统提交,要么一起执行成功,要么一起失败
3.事务不支持嵌套
```

- 事务的特性

```mysql
1.原子性:不可被拆分
2.一致性:要么一起执行成功,要么一起失败
3.隔离性:事务彼此之间没有关系
4.永久性:一旦执行成功,不可被修改
```

- 开启事务

```mysql
#事务指针对写的动作 insert  update  delete
start transaction;
insert into stuinfo values(null,'name',1,18,'city');  #会产生一个新的id
insert into score values(9,100,100);

#回滚
rollback;

#执行成功
commit;
```

- 自动提交事务

```mysql
#自动提交事务是一个机制
show variables like 'autocommit';
#修改自动提交的状态
set autocommit= 0|1

#事务只有在开启的状态下才能使用
#事务只能在innodb的引擎下才能使用,myisam中没有这个机制
```

- 事务的四大特性性质：

在写入或更新资料的过程中, 为保证事务 (transaction) 是正确可靠的, 所必须具备的四个特性 (ACID)：

1. 原子性 (Atomicity) ：

   - 事务中的所有操作, 要么全部完成, 要么全部不完成, 不会结束在中间某个环节。
   - 事务在执行过程中发生错误, 会被回滚 (Rollback) 到事务开始前的状态, 就像这个事务从来没有执行过一样。

2. 一致性 (Consistency)：

   在事务开始之前和事务结束以后, 数据库的完整性没有被破坏。
   这表示写入的资料必须完全符合所有的预设规则, 这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。

3. 隔离性 (Isolation)：

   数据库允许多个并发事务同时对其数据进行读写和修改的能力, 隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。
   事务隔离分为不同级别, 包括:

   1. 读取未提交 (Read uncommitted)

      - 所有事务都可以看到其他未提交事务的执行结果
      - 本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少
      - 该级别引发的问题是——脏读(Dirty Read)：读取到了未提交的数据

   2. 读提交 (read committed)

      - 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）

      - 它满足了隔离的简单定义：一个事务只能看见已经提交事务做的改变

      - 这种隔离级别出现的问题是: 不可重复读(Nonrepeatable Read)：

        不可重复读意味着我们在同一个事务中执行完全相同的 select 语句时可能看到不一样的结果。

        导致这种情况的原因可能有：

        - 有一个交叉的事务有新的commit，导致了数据的改变;
        - 一个数据库被多个实例操作时,同一事务的其他实例在该实例处理其间可能会有新的commit

   3. 可重复读 (repeatable read)

      - 这是MySQL的默认事务隔离级别
      - 它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行
      - 此级别可能出现的问题: 幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行
      - InnoDB 通过多版本并发控制 (MVCC，Multiversion Concurrency Control) 机制解决幻读问题；
      - InnoDB 还通过间隙锁解决幻读问题

   4. 串行化 (Serializable)

      - 这是最高的隔离级别
      - 它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之,它是在每个读的数据行上加上共享锁。MySQL锁总结
      - 在这个级别，可能导致大量的超时现象和锁竞争

4. 持久性 (Durability)：

   事务处理结束后, 对数据的修改就是永久的, 即便系统故障也不会丢失。

#### 20. 对象关系映射(Object Relational Mapping，简称ORM)

#### **21.** 如果请求页面不返回内容怎样处理？

+ 检查url路径有无问题

+ 检查数据库是否有相关数据

#### 22.进行关键字过滤的命令

grep -n '文本内容' 文件名

### 23.python2 与python3 区别

python2 xrange 

文件原来file python3open

python2 tab space python3混合会报错 python2tab相当与8个空格

print 是语句不加括号 python3是函数

raw_input 输进去什么类型就是什么类型 python3 统一字符串

exec语句被python3废弃，统一使用exec函数

python2 round返回float python3 round 是int类型

python2 两个对象任意比 python3只能同类型比较

python2 for循环会改变原有的变量值 python3佛如循环不会改变

### 24.简述一下websocket的工作原理及作用

WebSocket是HTML5下一种新的协议。它实现了浏览器与服务器全双工通信，能更好的节省服务器资源和带宽并达到实时通讯的目的。它与HTTP一样通过已建立的TCP连接来传输数据，但是它和HTTP最大不同是：

1.WebSocket是一种双向通信协议。在建立连接后，WebSocket服务器端和客户端都能主动发送数据给对方或向对方接收数据，就像Socket一样；

2.WebSocket需要像TCP一样，先建立连接，连接成功后才能相互通信

相对于传统HTTP每次请求-应答都需要客户端与服务端建立连接的模式，WebSocket是类似Socket的TCP长连接通讯模式。一旦WebSocket连接建立后，后续数据都以帧序列的形式传输。在客户端断开WebSocket连接或Server端中断连接前，不需要客户端和服务端重新发起连接请求。在海量并发及客户端与服务器交互负载流量大的情况下，极大的节省了网络带宽资源的消耗，有明显的性能优势，且客户端发送和接受消息是在同一个持久连接上发起，实时性优势明显

作用：Websocket解决了HTTP的这几个难题。首先，被动性，当服务器完成协议升级后（HTTP->Websocket），服务端就可以主动推送信息给客户端啦。解决了上面同步有延迟的问题。

解决服务器上消耗资源的问题：其实我们所用的程序是要经过两层代理的，即HTTP协议在Nginx等服务器的解析下，然后再传送给相应的Handler（php等）来处理。简单地说，我们有一个非常快速的 接线员（Nginx） ，他负责把问题转交给相应的 客服（Handler） 。Websocket就解决了这样一个难题，建立后，可以直接跟接线员建立持久连接，有信息的时候客服想办法通知接线员，然后接线员在统一转交给客户。

由于Websocket只需要一次HTTP握手，所以说整个通讯过程是建立在一次连接/状态中，也就避免了HTTP的非状态性，服务端会一直知道你的信息，直到你关闭请求，这样就解决了接线员要反复解析HTTP协议，还要查看identity info的信息。目前唯一的问题是：不兼容低版本的IE

 

 

#### **25.** 简述tcp和udp的工作原理和应用，为什么早期的QQ是用udp

TCP（Transmission Control Protocol），又称[传输控制协议](https://www.baidu.com/s?wd=传输控制协议&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)，

提供面向连接、可靠的字节流服务。在传输数据流前，双方会先建立一条虚拟的通信道。一个TCP连接必须要经过三次“对话”才能建立起来，一为请求连接，二为同步要求，三为确认发送。详细的讲，TCP连接为接受端的接收缓冲区设置滑动窗口，接收端只允许发送缓冲区能容纳的数据，在滑动窗口的基础上进行流量控制，以防止数据溢出缓冲区。接收端还会在接收时进行TCP数据校验，有错就放弃该分片，不确认其接收，使之超时重发。这就保证数据的准确性和可靠性，同时也相对增加数据量和传输时间

UDP（User Data Protocol）又称[用户数据报协议](https://www.baidu.com/s?wd=用户数据报协议&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)，是与TCP相对应的协议。

UDP协议是将网络数据量压缩成数据包的形式在网络中进行传输，是一种无连接的协议。使用UDP传输数据时，每个数据段都是一个独立的信息，包括完整的源地址和目的地，在网络上以任何可能的 路径传到目的地，因此，能否到达目的地，以及到达目的地的时间和内容的完整性都不能保证。不过UDP报头携带的信息比TCP的少的多，有更多的数据空间

 

由于QQ的服务器设计容量是海量级的应用，一台服务器要同时容纳十几万的并发连接，因此服务器端只有采用UDP协议与客户端进行通讯才能保证这种超大规模的服务。

QQ客户端之间的消息传送也采用了UDP模式，因为早期国内的网络环境非常复杂，而且很多用户采用的方式是通过代理服务器共享一条线路上网的方式，在这些复杂的情况下，客户端之间能彼此建立起来TCP连接的概率较小，严重影响传送信息的效率。而UDP包能够穿透大部分的代理服务器，因此QQ选择了UDP作为客户之间的主要通信协议。

当时网络的特点就是接入带宽很窄而且抖动特别厉害。所谓抖动可能是多方面的，例如延时突发性地暴增、也有可能是由于路由层面的变化突然导致路由黑洞，还各种等等等等的问题。TCP因为拥塞控制、保证有序等原因，在这种网络状态上对带宽的利用是非常低的。而且因为网络抖动的原因，应用层心跳超时（一般不依靠keepalive）应用层主动断掉socket之后TCP需要三次握手才能重新建立链接，一旦出现频繁的小抖动就会使得带宽利用更低。而等待四次挥手的时间，也会占用服务器上宝贵的资源

 

#### 27.浏览器请求服务器，我只想知道这个请求的请求体和请求头的大小怎么实现；请求体与请求头的格式是什么样的

#### HTTP/2的简述

HTTP/2是现行HTTP协议（HTTP/1.x）的替代，但它不是重写，HTTP方法/状态码/语义都与HTTP/1.x一样。HTTP/2基于SPDY3，专注于***\*性能\****，最大的一个目标是在用户和网站间只用一个连接

 

HTTP/2 的主要目标是通过支持完整的请求与响应复用来减少延迟，通过有效压缩 HTTP 标头字段将协议开销降至最低，同时增加对请求优先级和服务器推送的支持。 为达成这些目标，HTTP/2 还给我们带来了大量其他协议层面的辅助实现，例如新的流控制、错误处理和升级机制

 

#### 29.sqlite3库都在什么上面用过

#### 在安卓的app中怎么对接python的接口



#### nginx怎么和django项目对接



想面中软的给你们几个问题吧，这几天听下来结合出去面人大致问的问题。

### 深浅拷贝的区别

浅拷贝

copy模块里面的copy方法实现。

浅拷贝是对一个对象父级（外层）的拷贝，并不会拷贝子级（内部）。使用浅拷贝的时候，分为两种情况。

第一种，如果最外层的数据类型是可变的，比如说列表，字典等，浅拷贝会开启新的地址空间去存放。

第二种，如果最外层的数据类型是不可变的，比如元组，字符串等，浅拷贝对象的时候，还是引用对象的地址空间

copy模块里面的deepcopy方法实现。

深拷贝对一个对象是所有层次的拷贝（递归），内部和外部都会被拷贝过来。

深拷贝也分两种情况：

第一种，最外层数据类型可变。这个时候，内部和外部的都会拷贝过来。

第二种，外层数据类型不可变，如果里面是可变数据类型，会新开辟地址空间存放。如果内部数据类型不可变，才会如同浅拷贝一样，是对地址的引用

### 单例模式的实现方式

1. __new__特殊方法实现

```python
class Singleton:

  def __new__(cls, *args, **kwargs):

    if not hasattr(cls, '_instance'):

      cls._instance = super(Singleton, cls).__new__(cls)

    return cls._instance

  def __init__(self, name):

    self.name = name

s1 = Singleton('first')

s2= Singleton('last')

print(s1 == s2)

>> True

print(s1.name)

>> last
```

tips: __new__方法无法避免触发init()，初始的成员变量会进行覆盖

2. 装饰器实现

```python
def singleton(cls):

  _instance = {}

  def inner(*args, **kwargs):

    if cls not in _instance:

      _instance[cls] = cls(*args, **kwargs)

    return _instance[cls]

  return inner

@singleton

class Test:

  def __init__(self, name):

    self.name = name

t1 = Test('first')

t2 = Test('last')

print(t1==t2)

>> True

print(t2.name)

>> first
```



3. 类装饰器实现

```python
class Singleton:

  def __init__(self, cls):

    self._cls = cls

    self._instance = {}

  def __call__(self, *args):

    if self._cls not in self._instance:

      self._instance[self._cls] = self._cls(*args)

    return self._instance[self._cls]

@Singleton

class Cls2:

  def __init__(self, name):

    self.name = name

cls1 = Cls2('first')

cls2 = Cls2('last')

print(id(cls1) == id(cls2))

>> True

print(cls1.name)

>> first
```



4. 元类实现

```python
class Singleton1(type):
   
	def __init__(self, *args, **kwargs):
        self.__instance = None

    super(Singleton1, self).__init__(*args, **kwargs)

 

  	def __call__(self, *args, **kwargs):

    	if self.__instance is None:

      		self.__instance = super(Singleton1, self).__call__(*args, **kwargs)

   		return self.__instance

class C(metaclass=Singleton1):

  def __init__(self, name):

    self.name = name

c1 = C('first')

c2 = C('last')

print(c1 == c2)

>> True

print(c2.name)

>> first
```



5. 模块实现

Python 的模块就是天然的单例模式，因为模块在第一次导入时，会生成 .pyc 文件，当第二次导入时，就会直接加载 .pyc 文件，而不会再次执行模块代码。

```python
#foo1.py

class Singleton(object):

  def foo(self):

    pass

singleton = Singleton()

#foo.py

from foo1 import singleton
```



### 装饰器原理

 

### django框架简单介绍

Django框架相较于其他框架的优势就是：大而全，框架的本身本身集成了ORM、模型绑定、模板引擎、缓存、Session等诸多功能

Django框架本身也是一个MVC模型，但是在Django中，控制器接受用户输入的部分由框架自行处理，所以 Django 里更关注的是模型（Model）、模板(Template)和视图（Views），称为 MTV模式：

    M 代表模型（Model），即数据存取层。 该层处理与数据相关的所有事务： 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。
    
    T 代表模板(Template)，即表现层。 该层处理与表现相关的决定： 如何在页面或其他类型文档中进行显示。
    
    V 代表视图（View），即业务逻辑层。 该层包含存取模型及调取恰当模板的相关逻辑。 你可以把它看作模型与模板之间的桥梁


### setting的配置



### shell脚本



### 冒泡排序

```python
def bubble_sort(nums):

  for i in range(len(nums) - 1):

    for j in range(len(nums) - i - 1):

      if nums[j] > nums[j + 1]:

        nums[j], nums[j + 1] = nums[j + 1], nums[j]

return nums
```

优化版本

```python
def bubble_sort2(L):

    for j in range(len(L)-1, 0, -1):  # j =2,1
        count = 0
        for i in range(0, j):
            if L[i] > L[i + 1]:
                L[i], L[i + 1] = L[i + 1], L[i]

                count += 1

        if count == 0:

            return

L = [3, 7, 8]

bubble_sort2(L)

print(L)
```



### 迭代器生成器

#### 迭代器

迭代是Python最强大的功能之一，是访问集合元素的一种方式。

迭代器是一个可以记住遍历的位置的对象。

迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。

迭代器有两个基本的方法：iter() 和 next()。

迭代器的简单实现

```python
class MyNumbers:

 def __iter__(self):

  self.a = 1

  return self

 def __next__(self):

  if self.a <= 20:

   x = self.a

   self.a += 1

   return x

  else:

   raise StopIteration 

myclass = MyNumbers()

myiter = iter(myclass)

for x in myiter:

 print(x)
```

StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 __next__() 方法中我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代

#### 生成器

在 Python 中，使用了 yield 的函数被称为生成器（generator）。

跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。

在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。

调用一个生成器函数，返回的是一个迭代器对象

生成器的简单实现

```python
def create_counter(n):

    print("create_counter")
    while True:

    yield n

    print("increment n")
    	n +=1

gen = create_counter(2)

print(gen)

print(next(gen))

print(next(gen))
```

 

### scrapy框架

工作流程

1. 引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。

2. 引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。

3. 引擎向调度器请求下一个要爬取的URL。

4. 调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。

5. 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。

6. 引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。

7. Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。

8. 引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。

9. (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。

 

### 数据库中的读锁(共享锁)、写锁(排它锁)、乐观锁、悲观锁

共享锁(S锁)：共享 (S) 用于不更改或不更新数据的操作（只读操作），如 SELECT 语句。 共享锁就是允许多个线程同时获取一个锁，一个锁可以同时被多个线程拥有

如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。

语法：select * from table lock in share mode

 

排他锁（X锁）： 排它锁，也称作独占锁，一个锁在某一时刻只能被一个线程占有，其它线程必须等待锁被释放之后才可能获取到锁，用于数据修改操作，例如 INSERT、UPDATE 或 DELETE。确保不会同时同一资源进行多重更新。

如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。

语法：select * from table for update --增删改自动加了排他锁

乐观锁
乐观锁不是数据库自带的，需要我们自己去实现。乐观锁是指操作数据库时(更新操作)，想法很乐观，认为这次的操作不会导致冲突，在操作数据时，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了

实现方式：

version方式

一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。

sql实现代码：

 

update table set x=x+1, version=version+1 where id=#{id} and version=#{version}; 

悲观锁

总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）



运行gunicorn 

gunicorn -c 配置文件的路径(swiper/gunicorn-config.py) django中wsgi文件(swiper.wsgi)

+ meinheld 使纯C语言写的，是一个比gevent更快的一个协程库，是异步处理的
+ kill cat logs/gunicorn.pid 反引号的作用就是优先执行里面的命令

### 影响服务器性能的因素

硬件因素

+ CPU 
+ 内存 
+ 带宽 
+ 磁盘

软件因素 

+ 操作系统 
+ 系统内核参数 
+ 语言 
+ 程序并发模型

### redis 都是什么原因会出故障

### 单线程 nginx 怎么控制连接数量 

### MySQL引擎 

MyISAM
    该引擎基于ISAM数据库引擎，但同样不提供事务的支持，也不支持行级锁和外键。因此当执行Insert插入和Update更新语句时，即执行写操作的时候需要锁定这个表。所以会导致效率会降低。不过和Innodb不同的是，MyIASM引擎是保存了表的行数，于是当进行Select count(*) from table语句时，可以直接的读取已经保存的值而不需要进行扫描全表。所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的。可以将MyIASM作为数据库引擎的首选

Innodb引擎

Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。它本身实际上是基于Mysql后台的完整的系统。Mysql运行的时候，Innodb会在内存中建立缓冲池，用于缓冲数据和索引。但是，该引擎是不支持全文搜索的。同时，启动也比较的慢，它是不会保存表的行数的。当进行Select count(*) from table指令的时候，需要进行扫描全表。所以当需要使用数据库的事务时，该引擎就是首选。由于锁的粒度小，写操作是不会锁定全表的。所以在并发度较高的场景下使用会提升效率的。

HEAP(也称为MEMORY)
    该存储引擎通过在内存中创建临时表来存储数据。每个基于该存储引擎的表实际对应一个磁盘文件，该文件的文件名和表名是相同的，类型为.frm。该磁盘文件只存储表的结构，而其数据存储在内存中，所以使用该种引擎的表拥有极高的插入、更新和查询效率。这种存储引擎默认使用哈希（HASH）索引，其速度比使用B-+Tree型要快，但也可以使用B树型索引。由于这种存储引擎所存储的数据保存在内存中，所以其保存的数据具有不稳定性，比如如果mysqld进程发生异常、重启或计算机关机等等都会造成这些数据的消失，所以这种存储引擎中的表的生命周期很短，一般只使用一次

补充2点：

  1.大容量的数据集时趋向于选择Innodb。因为它支持事务处理和故障的恢复。Innodb可以利用数据日志来进行数据的恢复。主键的查询在Innodb也是比较快的。

  2.大批量的插入语句时（这里是INSERT语句）在MyIASM引擎中执行的比较的快，但是UPDATE语句在Innodb下执行的会比较的快，尤其是在并发量大的时候。

 

### 索引 

#### 聚焦索引 

索引中键值的逻辑顺序决定了表中相应行的物理顺序（索引中的数据物理存放地址和索引的顺序是一致的），可以这么理解：只要是索引是连续的，那么数据在存储介质上的存储位置也是连续的。
比方说：想要到字典上查找一个字，我们可以根据字典前面的拼音找到该字，注意拼音的排列是有顺序的

聚焦索引的创建

create CLUSTERED INDEX 索引名称 ON 表名(字段名)

#### 联合索引（最左原则） 

命名规则：表名_字段名

1、需要加索引的字段，要在where条件中

2、数据量少的字段不需要加索引

3、如果where条件中是OR关系，加索引不起作用

4、符合最左原则

联合索引又叫复合索引。对于复合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。例如索引是key index (a,b,c). 可以支持a | a,b| a,b,c 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效

不带id字段和带id字段的查询的区别， 

通过进程id查看相关所有的文件ls of 

查看一个文件中一层一层的所有的文件目录 

删除权限 

post get optpath sys. 

手动触发行锁

### 时间复杂度 

一般情况下，算法中的基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n),使得当n趋于无穷大的时候，T(n)f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同量级函数，记住T(n)=O(f(n))称O(f(n))为算法的渐进时间复杂度，简称时间复杂度

#### 时间复杂度计算

- 1.找出算法中的基本语句
   算法中执行次数最多那条居于就是基本语句，通常是内层循环的循环体
- 2.计算基本语句的执行次数的数量级
   只需计算基本语句执行次数的数量级，这就意味着只要保证基本语句执行次数的函数中的最高次幂正确即可
   可以忽略所有低次幂和最高次幂的常数
- 3.用大O记号表示算法的时间性能

### 接口的平均耗时 



### 负载 



### 写一个接口的时候，怎么跟前端沟通 



### 测试 

#### 裸测 

#### 自测 

#### timeit

测试一段代码的运行时间，在python里面有个很简单的方法，就是使用timeit模块，使用起来超级方便

下面简单介绍一个timeit模块中的函数

主要就是这两个函数：

1,   timeit(stmt='pass', setup='pass', timer=<defaulttimer>, number=1000000)

​    返回：

​      返回执行stmt这段代码number遍所用的时间，单位为秒，float型

​    参数：

​      stmt：要执行的那段代码

​      setup：执行代码的准备工作，不计入时间，一般是import之类的

​      timer：这个在win32下是time.clock()，linux下是time.time()，默认的，不用管

​      number：要执行stmt多少遍

2,   repeat(stmt='pass', setup='pass', timer=<defaulttimer>, repeat=3, number=1000000)

​    这个函数比timeit函数多了一个repeat参数而已，表示重复执行timeit这个过程多少遍，返回一个列表，表示执行每遍的时间

当然，为了方便，python还用了一个**Timer**类，Timer类里面的函数跟上面介绍的两个函数是一样一样的

class timeit.Timer(stmt='pass', setup='pass',timer=<timer function>)

​    Timer.timeit(number=1000000)

​    Timer.repeat(repeat=3,number=1000000) 

### 正则表达式 



### DNS解析用的UDP协议 

客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，这样DNS服务器负载更低，响应更快。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包

 

### 如何不影响运行情况下，删除日志记录 <file.log

 



### 版本管理工具 

#### 1.Git

Git是目前世界上最先进的分布式版本控制系统，使用Git和Gitlab搭建版本控制环境是现在互联网公司最流行的版本控制方式
简介：
Git是一个免费的开源分布式版本控制系统，旨在快速高效地处理从小型到大型项目的所有事务。
Git易于学习，占用内存小，具有闪电般快速的性能。

#### 2.SVN

简介：
TortoiseSVN是一款非常易于使用的跨平台的 版本控制/版本控制/源代码控制软件。它基于[Apache Subversion（SVN）®](https://links.jianshu.com/go?to=http://subversion.apache.org/); TortoiseSVN为Subversion提供了一个简单易用的用户界面。

#### 3.HG

简介：
Mercurial是一个免费的分布式源代码管理工具。它可以有效地处理任何规模的项目，并提供简单直观的界面。Mercurial 是一种轻量级分布式版本控制系统，采用 Python 语言实现，易于学习和使用，扩展性强。

#### 4.CVS

CVS是版本控制系统，是源配置管理（SCM）的重要组成部分。使用它，您可以记录源文件和文档的历史记录。老牌的版本控制系统，它是基于客户端/服务器的行为使得其可容纳多用户，构成网络也很方便。这一特性使得CVS成为位于不同地点的人同时处理数据文件（特别是程序的源代码）时的首选。

#### 5.BitKeeper

是由BitMover公司提供的，BitKeeper自称是“分布式”可扩缩SCM系统。

不是采用C/S结构，而是采用P2P结构来实现的，同样支持变更任务，所有变更集的操作都是原子的，与svn,cvs一致。

### 后端中间件判断用户是否是登录状态，是通过什么判断，判断后会设置黑白名单，限定用户指定权限访问指定资源 



### 递归

函数直接或者间接调用自身就是递归。

递归需要有边界条件、递归前进段、递归返回段。

递归一定要有边界条件。

当边界不满足的时候递归前进。

当边界条件满足的时候，递归返回。

#### 递归要求

递归一定要有退出条件，递归调用一定要执行到这个退出条件，没有退出条件的递归调用，就是无限调用。

递归调用的深度不宜过深。

Python中对递归调用的深度做了限制，以保护解释器。

超过递归调用深度，会抛出异常的，Python3.7.3 中默认递归深度为1000，若超过此递归深度将抛出异常 "RecursionError: maximum recursion depth exceeded while calling a Python object".